name: llama.cpp - ALL FEATURES MENTOK 2025 (Tools + Vision + Embedding + Insert + CURL)

on:
  workflow_dispatch:
  push:
    branches: [ master, main ]

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: windows-all-features
            os: windows-latest
            artifact: llama-win-ALL.exe

          - name: linux-all-features
            os: ubuntu-latest
            artifact: llama-linux-ALL

    runs-on: ${{ matrix.os }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install Linux dependencies
        if: runner.os == 'Linux'
        run: |
          sudo apt update -qq
          sudo apt install -y build-essential cmake ninja-build libcurl4-openssl-dev libvulkan-dev glslc libshaderc-dev

      - name: Setup MSVC (Windows)
        if: runner.os == 'Windows'
        uses: ilammy/msvc-dev-cmd@v1

      # CURL untuk Windows
      - name: Install vcpkg + curl (Windows only)
        if: runner.os == 'Windows'
        shell: bash
        run: |
          git clone --depth 1 https://github.com/microsoft/vcpkg.git
          ./vcpkg/bootstrap-vcpkg.bat
          ./vcpkg/vcpkg_integral install curl:x64-windows-static-md --triplet x64-windows-static-md
          echo "VCPKG_ROOT=$(pwd)/vcpkg" >> $GITHUB_ENV

      # SEMUA FITUR MENTOK + TOOLS + VISION + EMBEDDING + INSERT
      - name: CMake Configure - ALL FEATURES ON
        shell: bash
        run: |
          cmake -B build -G Ninja \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_BUILD_SERVER=ON \
            -DLLAMA_BUILD_EXAMPLES=ON \
            -DLLAMA_CURL=ON \
            -DLLAMA_VISION=ON \
            -DLLAMA_TOOL_USE=ON \
            -DLLAMA_INSERT=ON \
            -DLLAMA_EMBEDDING=ON \
            -DLLAMA_RPC=ON \
            -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON \
            -DGGML_AVX2=ON -DGGML_AVX=ON -DGGML_FMA=ON -DGGML_F16C=ON -DGGML_NATIVE=ON \
            ${{ runner.os == 'Windows' && '-DCMAKE_TOOLCHAIN_FILE="$VCPKG_ROOT/scripts/buildsystems/vcpkg.cmake" -DVCPKG_TARGET_TRIPLET=x64-windows-static-md' || '' }}

      - name: Build
        run: cmake --build build --config Release --parallel

      # UPX AUTO LATEST - FIX WINDOWS 100% AMAN (pake Python, gak pake grep/cut/sed)
      - name: Get latest UPX version (Windows + Linux safe)
        id: upx
        shell: bash
        run: |
          VER=$(curl -s https://api.github.com/repos/upx/upx/releases/latest | python -c "import sys,json; print(json.load(sys.stdin)['tag_name'][1:])")
          echo "version=$VER" >> $GITHUB_OUTPUT
          echo "UPX version detected: $VER"

      - name: Download UPX (Linux)
        if: runner.os == 'Linux'
        shell: bash
        run: |
          VER=${{ steps.upx.outputs.version }}
          curl -L -o upx.tar.xz https://github.com/upx/upx/releases/download/v$VER/upx-${VER}-amd64_linux.tar.xz
          tar -xJf upx.tar.xz --strip-components=1
          chmod +x upx

      - name: Download UPX (Windows)
        if: runner.os == 'Windows'
        shell: bash
        run: |
          VER=${{ steps.upx.outputs.version }}
          curl -L -o upx.zip https://github.com/upx/upx/releases/download/v$VER/upx-${VER}-win64.zip
          unzip -o upx.zip
          mv upx-*/upx.exe . 2>nul || mv upx.exe .

      - name: Compress with UPX (ultra)
        continue-on-error: true
        shell: bash
        run: |
          if [ "${{ runner.os }}" = "Windows" ]; then
            ./upx.exe --best --lzma "build/bin/Release/llama-server.exe" "build/bin/Release/llama-cli.exe" 2>nul || true
          else
            ./upx --best --lzma build/bin/llama-server build/bin/llama-cli 2>/dev/null || true
          fi

      - name: Prepare artifacts
        shell: bash
        run: |
          mkdir release
          if [ "${{ runner.os }}" = "Windows" ]; then
            cp build/bin/Release/llama-server.exe release/${{ matrix.artifact }}
            [ -f build/bin/Release/llama-cli.exe ] && cp build/bin/Release/llama-cli.exe release/llama-cli-${{ matrix.artifact }} || true
          else
            cp build/bin/llama-server release/${{ matrix.artifact }}
            [ -f build/bin/llama-cli ] && cp build/bin/llama-cli release/llama-cli-${{ matrix.artifact }} || true
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.name }}
          path: release/*
