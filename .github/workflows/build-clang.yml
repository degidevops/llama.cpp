name: llama.cpp - FULL 2025 (Tools + Vision + Embedding + FlashAttn + Optimasi Rata Kanan)

on:
  workflow_dispatch:
  push:
    branches: [ master, main ]

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        include:
          # === WINDOWS FULL ===
          - name: win-full-avx512
            os: windows-latest
            cmake_flags: "-DGGML_AVX512=ON -DGGML_AVX2=ON -DGGML_AVX=ON -DGGML_FMA=ON -DGGML_F16C=ON -DGGML_NATIVE=ON -DLLAMA_EMBEDDING=ON"
            artifact: llama-win-full-avx512.exe

          - name: win-full-cuda
            os: windows-latest
            cmake_flags: "-DGGML_CUDA=ON -DGGML_CUDA_FLASH_ATTN=ON -DGGML_CUDA_FORCE_CUBLAS=ON -DGGML_CUDA_F16=ON -DLLAMA_EMBEDDING=ON -DCMAKE_CUDA_ARCHITECTURES=native"
            artifact: llama-win-full-cuda.exe
            cuda: true

          # === LINUX FULL ===
          - name: linux-full-avx512
            os: ubuntu-latest
            cmake_flags: "-DGGML_AVX512=ON -DGGML_AVX2=ON -DGGML_AVX=ON -DGGML_FMA=ON -DGGML_F16C=ON -DGGML_NATIVE=ON -DLLAMA_EMBEDDING=ON -DLLAMA_RPC=ON"
            artifact: llama-linux-full-avx512

          - name: linux-full-cuda
            os: ubuntu-latest
            cmake_flags: "-DGGML_CUDA=ON -DGGML_CUDA_FLASH_ATTN=ON -DGGML_CUDA_FORCE_CUBLAS=ON -DGGML_CUDA_F16=ON -DLLAMA_EMBEDDING=ON -DCMAKE_CUDA_ARCHITECTURES=native"
            artifact: llama-linux-full-cuda
            cuda: true

          - name: linux-full-vulkan
            os: ubuntu-latest
            cmake_flags: "-DGGML_VULKAN=ON -DGGML_VULKAN_SHADERC=ON -DLLAMA_EMBEDDING=ON -DLLAMA_RPC=ON"
            artifact: llama-linux-full-vulkan

    runs-on: ${{ matrix.os }}

    steps:
      - name: Checkout llama.cpp with latest tools PRs (Nov 2025)
        uses: actions/checkout@v4
        with:
          repository: ggerganov/llama.cpp
          ref: master                     # sudah include semua PR tools/vision sampai Nov 2025
          submodules: recursive

      - name: Install Linux deps
        if: runner.os == 'Linux'
        run: |
          sudo apt update -qq
          sudo apt install -y build-essential cmake ninja-build libcurl4-openssl-dev libvulkan-dev glslc libshaderc-dev

      - name: Install CUDA 12.6.2
        if: matrix.cuda
        uses: Jimver/cuda-toolkit@v0.2.29
        with:
          cuda: "12.6.2"
          method: network

      - name: Setup MSVC
        if: runner.os == 'Windows'
        uses: ilammy/msvc-dev-cmd@v1

      - name: Install libcurl Windows (vcpkg)
        if: runner.os == 'Windows'
        shell: bash
        run: |
          git clone --depth 1 https://github.com/microsoft/vcpkg.git
          ./vcpkg/bootstrap-vcpkg.bat
          ./vcpkg/vcpkg install curl:x64-windows-static-md --triplet x64-windows-static-md
          echo "VCPKG_ROOT=$(pwd)/vcpkg" >> $GITHUB_ENV

      - name: CMake Configure FULL 2025
        shell: bash
        run: |
          cmake -B build -G Ninja \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_BUILD_SERVER=ON \
            -DLLAMA_BUILD_EXAMPLES=ON \
            -DLLAMA_CURL=ON \
            -DLLAMA_EMBEDDING=ON \
            ${{ runner.os == 'Windows' && '-DCMAKE_TOOLCHAIN_FILE="$VCPKG_ROOT/scripts/buildsystems/vcpkg.cmake" -DVCPKG_TARGET_TRIPLET=x64-windows-static-md' || '' }} \
            ${{ matrix.cmake_flags }}

      - name: Build
        run: cmake --build build --config Release --parallel

      # UPX Auto Forever (sama seperti sebelumnya, sudah fix Windows)
      - name: Get latest UPX
        id: upx
        shell: pwsh
        run: |
          $json = Invoke-WebRequest https://api.github.com/repos/upx/upx/releases/latest -UseBasicParsing | ConvertFrom-Json
          $ver = $json.tag_name -replace '^v',''
          "version=$ver" >> $env:GITHUB_OUTPUT

      - name: Download UPX Linux
        if: runner.os == 'Linux'
        run: |
          VER=${{ steps.upx.outputs.version }}
          curl -L -o upx.tar.xz https://github.com/upx/upx/releases/download/v$VER/upx-${VER}-amd64_linux.tar.xz
          tar -xJf upx.tar.xz --strip-components=1
          chmod +x upx

      - name: Download UPX Windows
        if: runner.os == 'Windows'
        shell: pwsh
        run: |
          $ver = "${{ steps.upx.outputs.version }}"
          Invoke-WebRequest "https://github.com/upx/upx/releases/download/v$ver/upx-$ver-win64.zip" -OutFile upx.zip
          Expand-Archive upx.zip . -Force
          Move-Item "upx-*-win64/upx.exe" upx.exe -Force

      - name: UPX Compress
        continue-on-error: true
        shell: bash
        run: |
          if [ "${{ runner.os }}" = "Windows" ]; then
            ./upx.exe --best --lzma build/bin/*.exe 2>nul || true
          else
            ./upx --best --lzma build/bin/llama-* build/bin/Release/llama-* 2>/dev/null || true
          fi

      - name: Prepare artifacts
        shell: bash
        run: |
          mkdir -p release
          if [ "${{ runner.os }}" = "Windows" ]; then
            cp build/bin/llama-server.exe release/${{ matrix.artifact }}
            [ -f build/bin/llama-cli.exe ] && cp build/bin/llama-cli.exe release/llama-cli-${{ matrix.artifact }} || true
          else
            cp build/bin/llama-server release/${{ matrix.artifact }}
            [ -f build/bin/llama-cli ] && cp build/bin/llama-cli release/llama-cli-${{ matrix.artifact }} || true
          fi

      - name: Upload
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.name }}
          path: release/*
